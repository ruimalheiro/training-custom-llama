# datasets_path
HF_PRETRAIN_DATASET = '<my_huggingface_dataset_checkpoint>'
HF_PRETRAIN_DATASET_NAME = '<my_dataset_name>' # Used 'default' otherwise
HF_PRETRAIN_DATASET_SPLIT = 'train'
HF_PRETRAIN_DATASET_TARGET_PATH = 'dataset_<my_dataset>'
HF_PRETRAIN_DATASET_SHARD_PREFIX = '<my_dataset>'

HF_INSTRUCT_DATASET = '<my_huggingface_dataset_checkpoint>'
HF_INSTRUCT_DATASET_SELECTOR_KEY = '<selector key property>' # Main property to select the object. E.g. 'conversation' which inside would have 'content', 'role'
HF_INSTRUCT_DATASET_NAME = '<my_dataset_name>' # Used 'default' otherwise
HF_INSTRUCT_DATASET_SPLIT = 'train'
HF_INSTRUCT_DATASET_TARGET_PATH = 'dataset_<my_dataset>'
HF_INSTRUCT_DATASET_SHARD_PREFIX = '<my_dataset>'

NUMBER_OF_CPU_PROCESSES = 16

PRETRAIN_DATALOADER_ROOT_PATH = '<the path where the shards are>'
INSTRUCT_DATALOADER_ROOT_PATH = '<the path where the data is>'

HELLASWAG_PATH = 'dataset_hellaswag'

# load path / save
PRETRAIN_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
PRETRAIN_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'

INSTRUCT_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
INSTRUCT_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'


SAVE_CHECKPOINTS = False

# wnb
WNB_ENABLED = False
WNB_PROJECT_NAME = 'my_wnb_project_name'

# tokenizer model path
TOKENIZER_CHECKPOINT_PATH = './tokenizer.model'

# train config
TOTAL_BATCH_SIZE = 524288 # Consider gradient acumulation steps / number of gpus in the node.
MAX_LR = 0.0018
MIN_LR = 0.00018
WARMUP_STEPS = 715
WEIGHT_DECAY = 0.1
MAX_STEPS = 38146
EARLY_STOPPING_PATIENCE = 3
EARLY_STOPPING_PATIENCE_SKIP_STEPS = 0
IS_INSTRUCT_TRAINING = False
IS_MODEL_DISTILLATION = False
DISTILLATION_TEMPERATURE = 2.0
TEACHER_MODEL_CHECKPOINT = '<some hf model checkpoint>' # Assumes a teacher model from transformers, loaded with AutoModelForCausalLM

# validation
VALIDATE_EVERY_X_STEPS = 200
VAL_STEPS = 100
HELLASWAG_EVERY_X_STEPS = 200
HELLASWAG_NUMBER_OF_EXAMPLES = 100
GENERATE_EVERY_X_STEPS = 200
MAX_TEST_GEN_LEN = 64

# test prompts
TEST_PRETRAIN_GENERATION_PROMPTS = ["I am a language model,", "Computers are", "Artificial Intelligence is", "I like", "I live in", "Where are", "There was"]
TEST_INSTRUCT_GENERATION_PROMPTS = ["Where is Paris?", "How much is 1 + 1?", "Tell me where the White House is located", "Is it better to eat fish or meat?", "Where is London?", "What is Facebook?", "Who are you?"]

# model architecture config
DIM = 768
N_LAYERS = 16
N_HEADS = 16
N_KV_HEADS = 8
VOCAB_SIZE = 128256
MULTIPLE_OF = 1024
FFN_DIM_MULTIPLIER = 1.0
NORM_EPS = 1e-05
ROPE_THETA = 500000.0
MAX_BATCH_SIZE = 4
MAX_SEQ_LEN = 1024
