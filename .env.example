# datasets_path
HF_PRETRAIN_DATASET = '<my_huggingface_dataset_checkpoint>'
HF_PRETRAIN_DATASET_NAME = '<my_dataset_name>' # Used 'default' otherwise
HF_PRETRAIN_DATASET_SPLIT = 'train'
HF_PRETRAIN_DATASET_TARGET_PATH = 'dataset_<my_dataset>'
HF_PRETRAIN_DATASET_SHARD_PREFIX = '<my_dataset>'

HF_INSTRUCT_DATASET = '<my_huggingface_dataset_checkpoint>'
HF_INSTRUCT_DATASET_SELECTOR_KEY = '<selector key property>' # Main property to select the object. E.g. 'conversation' which inside would have 'content', 'role'
HF_INSTRUCT_DATASET_NAME = '<my_dataset_name>' # Used 'default' otherwise
HF_INSTRUCT_DATASET_SPLIT = 'train'
HF_INSTRUCT_DATASET_TARGET_PATH = 'dataset_<my_dataset>'
HF_INSTRUCT_DATASET_SHARD_PREFIX = '<my_dataset>'

HF_DPO_DATASET = '<my_huggingface_dataset_checkpoint>'
HF_DPO_DATASET_NAME = '<my_dataset_name>' # Used 'default' otherwise
HF_DPO_DATASET_SPLIT = 'train'
HF_DPO_DATASET_TARGET_PATH = 'dataset_<my_dataset>'
HF_DPO_DATASET_SHARD_PREFIX = '<my_dataset>'

NUMBER_OF_CPU_PROCESSES = 16

PRETRAIN_DATALOADER_ROOT_PATH = '<the path where the data is>'
INSTRUCT_DATALOADER_ROOT_PATH = '<the path where the data is>'
DPO_DATALOADER_ROOT_PATH = '<the path where the data is>'

HELLASWAG_PATH = 'dataset_hellaswag'

# load path / save
PRETRAIN_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
PRETRAIN_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'

INSTRUCT_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
INSTRUCT_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'

DPO_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
DPO_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'


SAVE_CHECKPOINTS = False

# wnb
WNB_ENABLED = False
WNB_PROJECT_NAME = 'my_wnb_project_name'

# tokenizer model path
TOKENIZER_CHECKPOINT_PATH = './tokenizer.model' # Or huggingface checkpoint.
HUGGINGFACE_TOKENIZER = False # Set to True if loading huggingface tokenizer.

# train config
TRAINING_STAGE = 'pretrain' # (pretrain, instruct, dpo)
TOTAL_BATCH_SIZE = 524288 # (max_batch_size * max_seq_len * ddp_world_size * grad_accum_steps)
MAX_LR = 0.0018
MIN_LR = 0.00018
WARMUP_STEPS = 715
WEIGHT_DECAY = 0.1
MAX_STEPS = 38146
EARLY_STOPPING_PATIENCE = 3
EARLY_STOPPING_PATIENCE_SKIP_STEPS = 0
DPO_BETA = 0.1
IS_MODEL_DISTILLATION = False
DISTILLATION_TEMPERATURE = 2.0
TEACHER_MODEL_CHECKPOINT = '<some hf model checkpoint>' # Assumes a teacher model from transformers, loaded with AutoModelForCausalLM
LORA_ENABLED = False
LORA_RANK = 16
LORA_ALPHA = 8
LORA_DROPOUT = 0.05
LORA_TARGET_MODULES = ["wq", "wk", "wv", "wo", "w1", "w3"]

# validation
VALIDATE_EVERY_X_STEPS = 200
VAL_STEPS = 100
HELLASWAG_EVERY_X_STEPS = 200
HELLASWAG_NUMBER_OF_EXAMPLES = 100
GENERATE_EVERY_X_STEPS = 200
MAX_TEST_GEN_LEN = 64

# test prompts
TEST_PROMPTS_FILE = 'test_prompts.json' # Modify to use a different file if needed. Expected keys in the file ('pretrain', 'instruct', 'dpo')

# model architecture config
DIM = 768
N_LAYERS = 16
N_HEADS = 16
N_KV_HEADS = 8
MULTIPLE_OF = 1024
FFN_DIM_MULTIPLIER = 1.0
NORM_EPS = 1e-05
ROPE_THETA = 500000.0
MAX_BATCH_SIZE = 4
MAX_SEQ_LEN = 1024
