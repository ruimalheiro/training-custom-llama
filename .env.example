# third party envs
WANDB_API_KEY = '<api key>' # Weights & Biases API key
HF_TOKEN = '<access token>' # Hugging Face access token
HF_HOME = '<cache dir for anything hf download>' # will use ./cache as default

# datasets
HF_PRETRAIN_DATASET_MIX_FILE = 'hf_pretrain_datasets_mix.json'
HF_PRETRAIN_DATASET_TARGET_PATH = './datasets/pretrain'

HF_INSTRUCT_DATASET_MIX_FILE = 'hf_instruct_datasets_mix.json'
HF_INSTRUCT_DATASET_TARGET_PATH = './datasets/instruct'

HF_DPO_DATASET_MIX_FILE = 'hf_dpo_datasets_mix.json'
HF_DPO_DATASET_TARGET_PATH = './datasets/dpo'

HF_INCLUDE_SOURCE_ID = False # This is to include a property 'source' to identify source dataset

# processes and batch sizes
NUMBER_OF_CPU_PROCESSES = 16
MP_POOL_CHUNK_SIZE = 32
HF_MAP_BATCH_SIZE = 1000 # Hugging Face dataset map batch size
HF_MAP_WRITER_BATCH_SIZE = 1000 # Hugging Face default dataset map writer batch size

# torch profiler
TORCH_PROFILER_ENABLED = False
TORCH_PROFILER_SCHEDULE_SKIP_FIRST = 0
TORCH_PROFILER_SCHEDULE_WAIT = 1
TORCH_PROFILER_SCHEDULE_WARMUP = 1
TORCH_PROFILER_SCHEDULE_ACTIVE = 1
TORCH_PROFILER_SCHEDULE_REPEAT = 0
TORCH_PROFILER_TENSORBOARD_ENABLED = False
TORCH_PROFILER_TENSORBOARD_LOG_PATH = './tensorboard_logs'

# paths for dataloaders
PRETRAIN_DATALOADER_ROOT_PATH = './datasets/pretrain'
INSTRUCT_DATALOADER_ROOT_PATH = './datasets/instruct'
DPO_DATALOADER_ROOT_PATH = './datasets/dpo'

HELLASWAG_PATH = 'hellaswag'

# system prompt
SYSTEM_PROMPT = 'You are a helpful AI assistant'

# load path / save
PRETRAIN_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
PRETRAIN_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'

INSTRUCT_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
INSTRUCT_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'

DPO_LOAD_CHECKPOINTS_PATH = '<checkpoints path>'
DPO_SAVE_CHECKPOINTS_PATH = '<checkpoints path>'


SAVE_CHECKPOINTS = False

# wandb
WANDB_ENABLED = False
WANDB_PROJECT_NAME = 'my_wandb_project_name'

# tokenizer model path
TOKENIZER_CHECKPOINT_PATH = './tokenizer.model' # Or huggingface checkpoint.
HUGGINGFACE_TOKENIZER = False # Set to True if loading huggingface tokenizer.

# train config
TRAINING_STAGE = 'pretrain' # (pretrain, instruct, dpo)
TOTAL_BATCH_SIZE = 524288 # (max_batch_size * max_seq_len * ddp_world_size * grad_accum_steps)
MAX_LR = 0.0018
MIN_LR = 0.00018
WARMUP_STEPS = 715
WEIGHT_DECAY = 0.1
MAX_STEPS = 38146
EARLY_STOPPING_PATIENCE = 3
EARLY_STOPPING_PATIENCE_SKIP_STEPS = 0
DPO_BETA = 0.1
IS_MODEL_DISTILLATION = False
DISTILLATION_TEMPERATURE = 2.0
TEACHER_MODEL_CHECKPOINT = '<some hf model checkpoint>' # Assumes a teacher model from transformers, loaded with AutoModelForCausalLM
LORA_ENABLED = False
LORA_RANK = 16
LORA_ALPHA = 8
LORA_DROPOUT = 0.05
LORA_TARGET_MODULES = ["wq", "wk", "wv", "wo", "w1", "w3"]

# validation
VALIDATE_EVERY_X_STEPS = 200
VAL_STEPS = 100
HELLASWAG_EVERY_X_STEPS = 200
HELLASWAG_NUMBER_OF_EXAMPLES = 100
GENERATE_EVERY_X_STEPS = 200
MAX_TEST_GEN_LEN = 64

# test prompts
TEST_PROMPTS_FILE = 'test_prompts.json' # Modify to use a different file if needed. Expected keys in the file ('pretrain', 'instruct', 'dpo')

# model architecture config
DIM = 768
N_LAYERS = 16
N_HEADS = 16
N_KV_HEADS = 8
MULTIPLE_OF = 1024
FFN_DIM_MULTIPLIER = 1.0
NORM_EPS = 1e-05
ROPE_THETA = 500000.0
MAX_BATCH_SIZE = 4
MAX_SEQ_LEN = 1024
